<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="Context-Aware Human Behavior Prediction Using Multimodal Large Language Models: Challenges and Insights">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords"
    content="Human Behavior Prediction, Large Language Models, Vision Language Models, Artificial Intelligence, Machine Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>
    Context-Aware Human Behavior Prediction Using Multimodal Large Language Models: Challenges and Insights
  </title>

  <link rel="icon" type="image/x-icon" href="static/images/bosch_logo.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Context-Aware Human Behavior Prediction Using Multimodal Large
              Language Models: Challenges and Insights</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=dAfwuBQAAAAJ&hl=en" target="_blank">Yuchen
                  Liu</a><sup>1,2</sup>,</span>
              <span class="author-block">
                Lino Lerch</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=L-3aQNcAAAAJ&hl=en" target="_blank">Luigi
                  Palmieri</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=BlSfMYwAAAAJ&hl=en" target="_blank">Andrey
                  Rudenko</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=hX7EOUkAAAAJ&hl=en" target="_blank">Sebastian
                  Koch</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=FuY-lbcAAAAJ&hl=en" target="_blank">Timo
                  Ropinski</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=XvRUyU4AAAAJ&hl=en" target="_blank">Marco
                  Aiello</a><sup>2</sup>,</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Bosch Research,
                <sup>2</sup>University of Stuttgart, <sup>3</sup>Ulm University
              </span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2504.00839" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2504.00839" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/boschresearch/cap-mllm" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- News -->
  <section class="section" style="padding-top: 8px; margin-top: 0;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width has-text-centered">
          <p>ðŸ“¢ <strong> Accepted at IEEE International Conference on Robot and Human Interactive Communication
              (RO-MAN), 2025</strong> </p>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser image-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/cover_fig.drawio.png" alt="Cover fig" width="80%"
          style="display: block; margin-left: auto; margin-right: auto;" class="subtitle has-text-centered">
        <h2>
          Context-aware human behavior prediction using Multimodal Large Language Model (MLLM).
          The MLLM (shaded zone) is the main component of the prediction system.
          It consists of an LLM, a visual encoder, and an adapter (e.g., MLP, Q-Transformer,
          or cross-attention layers for generating tokens passed to the LLM).
          The MLLM takes the user instruction and the historical visual observation as input,
          and forecasts the human behavior.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser image -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Predicting human behavior in shared environments is crucial for safe and efficient human-robot
              interaction.
              Traditional data-driven methods to that end are pre-trained on domain-specific datasets, activity types,
              and prediction horizons.
              In contrast, the recent breakthroughs in Large Language Models (LLMs) promise open-ended
              cross-domain generalization to describe various human activities and make predictions in any context.
              In particular, Multimodal LLMs (MLLMs) are able to integrate information from various sources,
              achieving more contextual awareness and improved scene understanding.
              The difficulty in applying general-purpose MLLMs directly for prediction stems from their limited
              capacity for processing large input sequences, sensitivity to prompt design, and expensive fine-tuning.
              In this paper, we present a systematic analysis of applying pre-trained MLLMs for context-aware
              human behavior prediction. To this end, we introduce a modular multimodal human activity prediction
              framework that allows us to benchmark various MLLMs, input variations, In-Context Learning (ICL),
              and autoregressive techniques. Our evaluation indicates that the best-performing framework configuration
              is able to reach 92.8% semantic similarity and 66.1% exact label accuracy in predicting human behaviors
              in the target frame.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-3">System Architecture</h2>
        <img src="static/images/system_overview.drawio.png" alt="System overview"
          class="blend-img-background center-image">
        <p>
          <strong>System overview for human behavior prediction with scene context.</strong>
          The top part depicts the <strong>ground truth human interactions</strong>.
          The prediction system is illustrated in the lower part, which consists of an <strong>MLLM</strong>
          that is instructed with a <strong>prompt</strong> including the task description to predict future
          interaction labels and a set of In-Context Learning (ICL) examples.
          We ablate the visual input with four types of <strong>visual context representations</strong>:
          blind (no visual input), image, image sequence, and image caption, varying from one to three past
          time steps. An <strong>autoregressive prediction</strong> step is added to improve the predictions,
          which predicts the intermediate action labels.
        </p>
      </div>
    </div>
  </section>


  <!-- Hypotheses -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-3">Hypotheses</h2>
        <p>
          We formulate and evaluate several hypotheses on building a performant prediction system:
        </p>
        <p>
          <strong>H1:</strong> MLLMs are capable of correctly predicting non-trivial sequences of human activities
          (i.e., such where the target label is different from the latest observation, or where multiple correct
          activity labels are given as the ground truth).
        </p>
        <p>
          <strong>H2:</strong> Adding visual context yields more accurate predictions, compared to using purely
          text-based LLMs.
        </p>
        <p>
          <strong>H3:</strong> Prediction accuracy generally improves with the number of ICL examples, compared to
          zero-shot prediction.
        </p>
        <p>
          <strong>H4:</strong> Predicting intermediate actions between the current and the target time frame in an
          autoregressive manner improves the accuracy.
        </p>
        <br>
        <p>
          Our evaluation supports the hypotheses <strong>H1</strong>, <strong>H2</strong>, and <strong>H3</strong>.
          Firstly, many configurations can deliver competitive results without additional fine-tuning for correctly
          predicting non-trivial sequences of human activities. Furthermore, incorporating additional visual context and
          more ICL examples yields the best performance.
        </p>
      </div>
    </div>
  </section>
  <!-- End Hypotheses -->


  <!-- Prediction Examples -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-3">Examples of Human Behavior Predictions</h2>
        <div class="level-set has-text-justified">
          <img src="static/images/teaser_figure.drawio.png" alt="Prompt" width="80%"
            style="display: block; margin-left: auto; margin-right: auto;" class="blend-img-background center-image">
          <br>
          <p>
            Examples of human behavior predictions. The last row depicts a false prediction that differs from the ground
            truth.
          </p>
        </div>
      </div>
    </div>
  </section>
  <!-- End Prediction Examples -->


  <!-- Results -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container  is-max-desktop">
        <h2 class="title is-3">Results</h2>
        <div class="level-set has-text-justified">
          <img src="static/images/results_all.png" alt="Prompt" width="50%"
            style="display: block; margin-left: auto; margin-right: auto;" class="blend-img-background center-image">
          <p>
            Results of Zero-Shot Prediction. Higher values of accuracy and cosine similarity and lower value of edit
            distance indicate better performance. Bold numbers indicate the best results in each visual representation.
          </p>
          <br>
          <img src="static/images/GPT-4o.png" alt="Prompt" width="100%"
            style="display: block; margin-left: auto; margin-right: auto;" class="blend-img-background center-image">
          <img src="static/images/GPT-4o-mini.png" alt="Prompt" width="100%"
            style="display: block; margin-left: auto; margin-right: auto;" class="blend-img-background center-image">
          <img src="static/images/Qwen2-VL-72B.png" alt="Prompt" width="100%"
            style="display: block; margin-left: auto; margin-right: auto;" class="blend-img-background center-image">
          <img src="static/images/Qwen2-VL-7B.png" alt="Prompt" width="100%"
            style="display: block; margin-left: auto; margin-right: auto;" class="blend-img-background center-image">
          <img src="static/images/LLaVA-34B.png" alt="Prompt" width="100%"
            style="display: block; margin-left: auto; margin-right: auto;" class="blend-img-background center-image">
          <img src="static/images/LLaVA-7B.png" alt="Prompt" width="100%"
            style="display: block; margin-left: auto; margin-right: auto;" class="blend-img-background center-image">
          <p>
            * LLaVA models seem to be unsuitable for ICL. This is potentially caused by its prompt template that always
            first renders all images before processing the text. The missing tokenize-in-place ability leads to the
            mismatching of example and query images, therefore consistently resulting in failures and generating
            corrupted output tokens.
            (Source: <a href="https://huggingface.co/llava-hf/LLaVA-NeXT-Video-7B-hf/discussions/3"
              target="_blank">https://huggingface.co/llava-hf/LLaVA-NeXT-Video-7B-hf/discussions/3</a>).
          </p>
        </div>
      </div>
    </div>
  </section>
  <!-- End Results -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
  @article{liu2025context,
    title={Context-aware human behavior prediction using multimodal large language models: Challenges and insights},
    author={Liu, Yuchen and Lerch, Lino and Palmieri, Luigi and Rudenko, Andrey and Koch, Sebastian and Ropinski, Timo and Aiello, Marco},
    journal={arXiv preprint arXiv:2504.00839},
    year={2025}
  }
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This is a private page, opinions are my own, results have been obtained during my PhD research. Yuchen
              Liu.<br>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>